{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(\n",
    "    tensor, num_encoding_functions=6, include_input=True, log_sampling=True\n",
    ") -> torch.Tensor:\n",
    "  r\"\"\"Apply positional encoding to the input.\n",
    "\n",
    "  Args:\n",
    "    tensor (torch.Tensor): Input tensor to be positionally encoded.\n",
    "    num_encoding_functions (optional, int): Number of encoding functions used to\n",
    "        compute a positional encoding (default: 6).\n",
    "    include_input (optional, bool): Whether or not to include the input in the\n",
    "        computed positional encoding (default: True).\n",
    "    log_sampling (optional, bool): Sample logarithmically in frequency space, as\n",
    "        opposed to linearly (default: True).\n",
    "\n",
    "  Returns:\n",
    "    (torch.Tensor): Positional encoding of the input tensor.\n",
    "  \"\"\"\n",
    "  # TESTED\n",
    "  # Trivially, the input tensor is added to the positional encoding.\n",
    "  encoding = [tensor] if include_input else []\n",
    "  # Now, encode the input using a set of high-frequency functions and append the\n",
    "  # resulting values to the encoding.\n",
    "  frequency_bands = None\n",
    "  if log_sampling:\n",
    "      frequency_bands = 2.0 ** torch.linspace(\n",
    "          0.0,\n",
    "          num_encoding_functions - 1,\n",
    "          num_encoding_functions,\n",
    "          dtype=tensor.dtype,\n",
    "          device=tensor.device,\n",
    "      )\n",
    "  else:\n",
    "      frequency_bands = torch.linspace(\n",
    "          2.0 ** 0.0,\n",
    "          2.0 ** (num_encoding_functions - 1),\n",
    "          num_encoding_functions,\n",
    "          dtype=tensor.dtype,\n",
    "          device=tensor.device,\n",
    "      )\n",
    "\n",
    "  for freq in frequency_bands:\n",
    "      for func in [torch.sin, torch.cos]:\n",
    "          encoding.append(func(tensor * freq))\n",
    "\n",
    "  # Special case, for no positional encoding\n",
    "  if len(encoding) == 1:\n",
    "      return encoding[0]\n",
    "  else:\n",
    "      return torch.cat(encoding, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryTinyNerfModel(torch.nn.Module):\n",
    "  r\"\"\"Define a \"very tiny\" NeRF model comprising three fully connected layers.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, filter_size=128, num_encoding_functions=6):\n",
    "    super(VeryTinyNerfModel, self).__init__()\n",
    "    # Input layer (default: 39 -> 128)\n",
    "    self.layer1 = torch.nn.Linear(\n",
    "        3 + 3 * 2 * num_encoding_functions, filter_size)\n",
    "    # Hidden layers\n",
    "    self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
    "    self.layer3 = torch.nn.Linear(filter_size, filter_size)\n",
    "    # Output layer\n",
    "    self.layer4 = torch.nn.Linear(filter_size, 3)\n",
    "    # Short hand for torch.nn.functional.relu\n",
    "    self.relu = torch.nn.functional.relu\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.relu(self.layer1(x))\n",
    "    x = self.relu(self.layer2(x))\n",
    "    x = self.relu(self.layer3(x))\n",
    "    x = self.layer4(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(inputs: torch.Tensor, chunksize: Optional[int] = 1024 * 8):\n",
    "  r\"\"\"Takes a huge tensor (ray \"bundle\") and splits it into a list of minibatches.\n",
    "  Each element of the list (except possibly the last) has dimension `0` of length\n",
    "  `chunksize`.\n",
    "  \"\"\"\n",
    "  return [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"../datasets/bunny/BigBuckBunny_320x180.mp4\")\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) \n",
    "total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "\n",
    "\n",
    "def get_frame(idx):\n",
    "  r\"\"\" Get the RGB tensor of a specific frame in the video.\n",
    "  \"\"\"\n",
    "  cap.set(cv2.CV_CAP_PROP_POS_MSEC, idx)\n",
    "  success, img = cap.read()\n",
    "  if not success:\n",
    "    print(\"Failed to load frame at index \" + str(idx))\n",
    "\n",
    "  return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xs_and_ys(width, height, frame_ind):\n",
    "    r\"\"\" Construct (x, y, f) tuples.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            res.append((y, x, frame_ind))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_iter_npc(width, height, frame_ind, encoding_fn, get_minibatches_fn, chunksize):\n",
    "\n",
    "  pts = xs_and_ys(width, height, frame_ind)\n",
    "\n",
    "  encoded_pts = encoding_fn(pts)\n",
    "  batches = get_minibatches_fn(encoded_pts, chunksize=chunksize)\n",
    "\n",
    "  predictions = []\n",
    "  for batch in batches:\n",
    "    predictions.append(model(batch))\n",
    "  \n",
    "  rgb_flat = torch.cat(predictions, dim=0)\n",
    "  rgb = torch.reshape(rgb_flat, [height, width, 3])\n",
    "  return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_video():\n",
    "  r\"\"\"Build the final video from the trained model.\"\"\"\n",
    "  out = cv2.VideoWriter('output_video.avi', cv2.VideoWriter_fourcc(*'DIVX'), fps, (width, height))\n",
    "  for f in range(total_frames):\n",
    "      rgb_predicted = one_iter_npc(width, height,\n",
    "                                   f, encode,\n",
    "                                   get_minibatches, chunksize)\n",
    "      out.write(rgb_predicted)\n",
    "  \n",
    "  out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters for NPC training\n",
    "\"\"\"\n",
    "\n",
    "# Number of functions used in the positional encoding (Be sure to update the\n",
    "# model if this number changes).\n",
    "num_encoding_functions = 6\n",
    "\n",
    "# Specify encoding function.\n",
    "def encode(x): return positional_encoding(\n",
    "    x, num_encoding_functions=num_encoding_functions)\n",
    "\n",
    "# Chunksize (Note: this isn't batchsize in the conventional sense. This only\n",
    "# specifies the number of rays to be queried in one go. Backprop still happens\n",
    "# only after all rays from the current \"bundle\" are queried and rendered).\n",
    "# Use chunksize of about 4096 to fit in ~1.4 GB of GPU memory.\n",
    "chunksize = 16384\n",
    "\n",
    "# Optimizer parameters\n",
    "lr = 5e-3\n",
    "num_iters = 1000\n",
    "\n",
    "# Misc parameters\n",
    "display_every = 100  # Number of iters after which stats are displayed\n",
    "\n",
    "\"\"\"\n",
    "Model\n",
    "\"\"\"\n",
    "model = VeryTinyNerfModel(num_encoding_functions=num_encoding_functions)\n",
    "model.to(device)\n",
    "\n",
    "\"\"\"\n",
    "Optimizer\n",
    "\"\"\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\"\"\"\n",
    "Train-Eval-Repeat!\n",
    "\"\"\"\n",
    "\n",
    "# Seed RNG, for repeatability\n",
    "seed = 1337\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Lists to log metrics etc.\n",
    "psnrs = []\n",
    "iternums = []\n",
    "\n",
    "for i in range(num_iters):\n",
    "\n",
    "  # Randomly pick a frame as the target\n",
    "  target_frame_idx = np.random.randint(total_frames)\n",
    "  target_img = get_frame(target_frame_idx)\n",
    "\n",
    "  rgb_predicted = one_iter_npc(width, height,\n",
    "                               target_frame_idx, encode,\n",
    "                               get_minibatches, chunksize)\n",
    "\n",
    "  # Compute mean-squared error between the predicted and target images. Backprop!\n",
    "  loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Display images/plots/stats\n",
    "  if i % display_every == 0:\n",
    "    test_frame = 15\n",
    "    # Render the held-out view\n",
    "    rgb_predicted = one_iter_npc(width, height,\n",
    "                                test_frame, encode,\n",
    "                                get_minibatches, chunksize)\n",
    "\n",
    "    loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
    "    print(\"Loss:\", loss.item())\n",
    "    psnr = -10. * torch.log10(loss)\n",
    "\n",
    "    psnrs.append(psnr.item())\n",
    "    iternums.append(i)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(rgb_predicted.detach().cpu().numpy())\n",
    "    plt.title(f\"Iteration {i}\")\n",
    "    plt.subplot(122)\n",
    "    plt.plot(iternums, psnrs)\n",
    "    plt.title(\"PSNR\")\n",
    "    plt.show()\n",
    "\n",
    "print('Done!')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
